{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "999e9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import html\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e24b08f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 217 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-macosx_10_11_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 395 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: requests in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.8)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "Successfully installed tokenizers-0.13.2 transformers-4.26.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b91fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: tqdm in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.63.0)\r\n",
      "Requirement already satisfied: click in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from nltk) (8.0.4)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from nltk) (2022.10.31)\r\n",
      "Requirement already satisfied: joblib in /Users/rubenhayat/opt/anaconda3/lib/python3.8/site-packages (from nltk) (1.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6cc4de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ucberkeley-dlab--measuring-hate-speech-c32713cabe528196\n",
      "Found cached dataset parquet (/Users/rubenhayat/.cache/huggingface/datasets/ucberkeley-dlab___parquet/ucberkeley-dlab--measuring-hate-speech-c32713cabe528196/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c979d1d1eb164e88b1a52b75eeaf3f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>platform</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>respect</th>\n",
       "      <th>insult</th>\n",
       "      <th>humiliate</th>\n",
       "      <th>status</th>\n",
       "      <th>dehumanize</th>\n",
       "      <th>violence</th>\n",
       "      <th>...</th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>hate_speech_score</th>\n",
       "      <th>infitms</th>\n",
       "      <th>outfitms</th>\n",
       "      <th>annotator_severity</th>\n",
       "      <th>std_err</th>\n",
       "      <th>annotator_infitms</th>\n",
       "      <th>annotator_outfitms</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>annotator_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.00000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135451.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23530.416138</td>\n",
       "      <td>5567.097812</td>\n",
       "      <td>1.281352</td>\n",
       "      <td>2.954307</td>\n",
       "      <td>2.828875</td>\n",
       "      <td>2.56331</td>\n",
       "      <td>2.278638</td>\n",
       "      <td>2.698575</td>\n",
       "      <td>1.846211</td>\n",
       "      <td>1.052045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.744733</td>\n",
       "      <td>-0.567428</td>\n",
       "      <td>1.034322</td>\n",
       "      <td>1.001052</td>\n",
       "      <td>-0.018817</td>\n",
       "      <td>0.300588</td>\n",
       "      <td>1.007158</td>\n",
       "      <td>1.011841</td>\n",
       "      <td>0.014589</td>\n",
       "      <td>37.910772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12387.194125</td>\n",
       "      <td>3230.508937</td>\n",
       "      <td>1.023542</td>\n",
       "      <td>1.231552</td>\n",
       "      <td>1.309548</td>\n",
       "      <td>1.38983</td>\n",
       "      <td>1.370876</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>1.402372</td>\n",
       "      <td>1.345706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.932260</td>\n",
       "      <td>2.380003</td>\n",
       "      <td>0.496867</td>\n",
       "      <td>0.791943</td>\n",
       "      <td>0.487261</td>\n",
       "      <td>0.236380</td>\n",
       "      <td>0.269876</td>\n",
       "      <td>0.675863</td>\n",
       "      <td>0.613006</td>\n",
       "      <td>11.641276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.340000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>-1.820000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>-1.578693</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18148.000000</td>\n",
       "      <td>2719.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.330000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>-0.380000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>-0.341008</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20052.000000</td>\n",
       "      <td>5602.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.340000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.110405</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32038.250000</td>\n",
       "      <td>8363.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.410000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>0.449555</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50070.000000</td>\n",
       "      <td>11142.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>2.010000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.987511</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          comment_id   annotator_id       platform      sentiment  \\\n",
       "count  135556.000000  135556.000000  135556.000000  135556.000000   \n",
       "mean    23530.416138    5567.097812       1.281352       2.954307   \n",
       "std     12387.194125    3230.508937       1.023542       1.231552   \n",
       "min         1.000000       1.000000       0.000000       0.000000   \n",
       "25%     18148.000000    2719.000000       0.000000       2.000000   \n",
       "50%     20052.000000    5602.500000       1.000000       3.000000   \n",
       "75%     32038.250000    8363.000000       2.000000       4.000000   \n",
       "max     50070.000000   11142.000000       3.000000       4.000000   \n",
       "\n",
       "             respect        insult      humiliate         status  \\\n",
       "count  135556.000000  135556.00000  135556.000000  135556.000000   \n",
       "mean        2.828875       2.56331       2.278638       2.698575   \n",
       "std         1.309548       1.38983       1.370876       0.898500   \n",
       "min         0.000000       0.00000       0.000000       0.000000   \n",
       "25%         2.000000       2.00000       1.000000       2.000000   \n",
       "50%         3.000000       3.00000       3.000000       3.000000   \n",
       "75%         4.000000       4.00000       3.000000       3.000000   \n",
       "max         4.000000       4.00000       4.000000       4.000000   \n",
       "\n",
       "          dehumanize       violence  ...     hatespeech  hate_speech_score  \\\n",
       "count  135556.000000  135556.000000  ...  135556.000000      135556.000000   \n",
       "mean        1.846211       1.052045  ...       0.744733          -0.567428   \n",
       "std         1.402372       1.345706  ...       0.932260           2.380003   \n",
       "min         0.000000       0.000000  ...       0.000000          -8.340000   \n",
       "25%         1.000000       0.000000  ...       0.000000          -2.330000   \n",
       "50%         2.000000       0.000000  ...       0.000000          -0.340000   \n",
       "75%         3.000000       2.000000  ...       2.000000           1.410000   \n",
       "max         4.000000       4.000000  ...       2.000000           6.300000   \n",
       "\n",
       "             infitms       outfitms  annotator_severity        std_err  \\\n",
       "count  135556.000000  135556.000000       135556.000000  135556.000000   \n",
       "mean        1.034322       1.001052           -0.018817       0.300588   \n",
       "std         0.496867       0.791943            0.487261       0.236380   \n",
       "min         0.100000       0.070000           -1.820000       0.020000   \n",
       "25%         0.710000       0.560000           -0.380000       0.030000   \n",
       "50%         0.960000       0.830000           -0.020000       0.340000   \n",
       "75%         1.300000       1.220000            0.350000       0.420000   \n",
       "max         5.900000       9.000000            1.360000       1.900000   \n",
       "\n",
       "       annotator_infitms  annotator_outfitms     hypothesis  annotator_age  \n",
       "count      135556.000000       135556.000000  135556.000000  135451.000000  \n",
       "mean            1.007158            1.011841       0.014589      37.910772  \n",
       "std             0.269876            0.675863       0.613006      11.641276  \n",
       "min             0.390000            0.280000      -1.578693      18.000000  \n",
       "25%             0.810000            0.670000      -0.341008      29.000000  \n",
       "50%             0.970000            0.850000       0.110405      35.000000  \n",
       "75%             1.170000            1.130000       0.449555      45.000000  \n",
       "max             2.010000            9.000000       0.987511      81.000000  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets \n",
    "dataset = datasets.load_dataset('ucberkeley-dlab/measuring-hate-speech', 'binary')   \n",
    "df = dataset['train'].to_pandas()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aebca28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in ./opt/anaconda3/lib/python3.8/site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./opt/anaconda3/lib/python3.8/site-packages (from datasets) (4.63.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./opt/anaconda3/lib/python3.8/site-packages (from datasets) (2.27.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-macosx_10_9_x86_64.whl (35 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 29.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "\u001b[K     |████████████████████████████████| 143 kB 20.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp38-cp38-macosx_10_9_x86_64.whl (359 kB)\n",
      "\u001b[K     |████████████████████████████████| 359 kB 50.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in ./opt/anaconda3/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: packaging in ./opt/anaconda3/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
      "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
      "\u001b[K     |████████████████████████████████| 190 kB 33.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting dill<0.3.7\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[K     |████████████████████████████████| 110 kB 46.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in ./opt/anaconda3/lib/python3.8/site-packages (from datasets) (1.22.3)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-11.0.0-cp38-cp38-macosx_10_14_x86_64.whl (24.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.4 MB 43.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in ./opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp38-cp38-macosx_10_9_x86_64.whl (29 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-macosx_10_9_x86_64.whl (36 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp38-cp38-macosx_10_9_x86_64.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 27.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in ./opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.1.1)\n",
      "Requirement already satisfied: filelock in ./opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./opt/anaconda3/lib/python3.8/site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./opt/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./opt/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in ./opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: multidict, frozenlist, yarl, async-timeout, aiosignal, fsspec, dill, aiohttp, xxhash, responses, pyarrow, multiprocess, huggingface-hub, datasets\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.9.0 dill-0.3.6 frozenlist-1.3.3 fsspec-2023.1.0 huggingface-hub-0.12.0 multidict-6.0.4 multiprocess-0.70.14 pyarrow-11.0.0 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1ced37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>platform</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>respect</th>\n",
       "      <th>insult</th>\n",
       "      <th>humiliate</th>\n",
       "      <th>status</th>\n",
       "      <th>dehumanize</th>\n",
       "      <th>violence</th>\n",
       "      <th>...</th>\n",
       "      <th>annotator_religion_hindu</th>\n",
       "      <th>annotator_religion_jewish</th>\n",
       "      <th>annotator_religion_mormon</th>\n",
       "      <th>annotator_religion_muslim</th>\n",
       "      <th>annotator_religion_nothing</th>\n",
       "      <th>annotator_religion_other</th>\n",
       "      <th>annotator_sexuality_bisexual</th>\n",
       "      <th>annotator_sexuality_gay</th>\n",
       "      <th>annotator_sexuality_straight</th>\n",
       "      <th>annotator_sexuality_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47777</td>\n",
       "      <td>10873</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39773</td>\n",
       "      <td>2790</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47101</td>\n",
       "      <td>3379</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43625</td>\n",
       "      <td>7365</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12538</td>\n",
       "      <td>488</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135551</th>\n",
       "      <td>37080</td>\n",
       "      <td>8590</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135552</th>\n",
       "      <td>22986</td>\n",
       "      <td>8303</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135553</th>\n",
       "      <td>21008</td>\n",
       "      <td>6207</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135554</th>\n",
       "      <td>22986</td>\n",
       "      <td>7886</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135555</th>\n",
       "      <td>14785</td>\n",
       "      <td>6897</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135556 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        comment_id  annotator_id  platform  sentiment  respect  insult  \\\n",
       "0            47777         10873         3        0.0      0.0     0.0   \n",
       "1            39773          2790         2        0.0      0.0     0.0   \n",
       "2            47101          3379         3        4.0      4.0     4.0   \n",
       "3            43625          7365         3        2.0      3.0     2.0   \n",
       "4            12538           488         0        4.0      4.0     4.0   \n",
       "...            ...           ...       ...        ...      ...     ...   \n",
       "135551       37080          8590         2        1.0      1.0     0.0   \n",
       "135552       22986          8303         2        2.0      0.0     0.0   \n",
       "135553       21008          6207         2        1.0      1.0     1.0   \n",
       "135554       22986          7886         2        2.0      0.0     0.0   \n",
       "135555       14785          6897         0        4.0      4.0     4.0   \n",
       "\n",
       "        humiliate  status  dehumanize  violence  ...  \\\n",
       "0             0.0     2.0         0.0       0.0  ...   \n",
       "1             0.0     2.0         0.0       0.0  ...   \n",
       "2             4.0     4.0         4.0       0.0  ...   \n",
       "3             1.0     2.0         0.0       0.0  ...   \n",
       "4             4.0     4.0         4.0       4.0  ...   \n",
       "...           ...     ...         ...       ...  ...   \n",
       "135551        0.0     2.0         0.0       0.0  ...   \n",
       "135552        0.0     2.0         0.0       0.0  ...   \n",
       "135553        1.0     1.0         0.0       0.0  ...   \n",
       "135554        0.0     2.0         0.0       0.0  ...   \n",
       "135555        2.0     2.0         2.0       3.0  ...   \n",
       "\n",
       "        annotator_religion_hindu  annotator_religion_jewish  \\\n",
       "0                          False                      False   \n",
       "1                          False                      False   \n",
       "2                          False                      False   \n",
       "3                          False                      False   \n",
       "4                          False                      False   \n",
       "...                          ...                        ...   \n",
       "135551                     False                      False   \n",
       "135552                     False                      False   \n",
       "135553                     False                      False   \n",
       "135554                     False                      False   \n",
       "135555                     False                      False   \n",
       "\n",
       "        annotator_religion_mormon  annotator_religion_muslim  \\\n",
       "0                           False                      False   \n",
       "1                           False                      False   \n",
       "2                           False                      False   \n",
       "3                           False                      False   \n",
       "4                           False                      False   \n",
       "...                           ...                        ...   \n",
       "135551                      False                      False   \n",
       "135552                      False                      False   \n",
       "135553                      False                      False   \n",
       "135554                      False                      False   \n",
       "135555                      False                      False   \n",
       "\n",
       "       annotator_religion_nothing  annotator_religion_other  \\\n",
       "0                           False                     False   \n",
       "1                           False                     False   \n",
       "2                            True                     False   \n",
       "3                           False                     False   \n",
       "4                           False                     False   \n",
       "...                           ...                       ...   \n",
       "135551                      False                     False   \n",
       "135552                      False                      True   \n",
       "135553                      False                     False   \n",
       "135554                       True                     False   \n",
       "135555                      False                     False   \n",
       "\n",
       "        annotator_sexuality_bisexual  annotator_sexuality_gay  \\\n",
       "0                              False                    False   \n",
       "1                              False                    False   \n",
       "2                              False                    False   \n",
       "3                              False                    False   \n",
       "4                              False                    False   \n",
       "...                              ...                      ...   \n",
       "135551                         False                    False   \n",
       "135552                          True                    False   \n",
       "135553                         False                    False   \n",
       "135554                         False                    False   \n",
       "135555                         False                    False   \n",
       "\n",
       "        annotator_sexuality_straight  annotator_sexuality_other  \n",
       "0                               True                      False  \n",
       "1                               True                      False  \n",
       "2                               True                      False  \n",
       "3                               True                      False  \n",
       "4                               True                      False  \n",
       "...                              ...                        ...  \n",
       "135551                          True                      False  \n",
       "135552                         False                      False  \n",
       "135553                          True                      False  \n",
       "135554                          True                      False  \n",
       "135555                          True                      False  \n",
       "\n",
       "[135556 rows x 131 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba77c5c",
   "metadata": {},
   "source": [
    "### Creating a new dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8425161",
   "metadata": {},
   "source": [
    "#### Check if same hate_speech_score for same comment_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "899bf670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_score_for_same_comments(df):\n",
    "    \"\"\"\n",
    "    Check if the values of hate_speech_score are the same for a same comment_id. \n",
    "    The function return all the id_comment where it is not the same.\n",
    "    \"\"\"\n",
    "    inconsistent_rows = []\n",
    "    all_id_comment = np.unique(df[\"comment_id\"])\n",
    "    for id_comment in all_id_comment:\n",
    "        list_score_hate_speech = np.array(df.loc[df[\"comment_id\"]==id_comment][\"hate_speech_score\"])\n",
    "        \n",
    "        if np.all(list_score_hate_speech != list_score_hate_speech[0]) : \n",
    "            inconsistent_rows.append(id_comment)\n",
    "            \n",
    "    return inconsistent_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d2a158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_score_for_same_comments(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ca66a",
   "metadata": {},
   "source": [
    "#### Liste des colonnes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d409f4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comment_id',\n",
       " 'annotator_id',\n",
       " 'platform',\n",
       " 'sentiment',\n",
       " 'respect',\n",
       " 'insult',\n",
       " 'humiliate',\n",
       " 'status',\n",
       " 'dehumanize',\n",
       " 'violence',\n",
       " 'genocide',\n",
       " 'attack_defend',\n",
       " 'hatespeech',\n",
       " 'hate_speech_score',\n",
       " 'text',\n",
       " 'infitms',\n",
       " 'outfitms',\n",
       " 'annotator_severity',\n",
       " 'std_err',\n",
       " 'annotator_infitms',\n",
       " 'annotator_outfitms',\n",
       " 'hypothesis',\n",
       " 'target_race_asian',\n",
       " 'target_race_black',\n",
       " 'target_race_latinx',\n",
       " 'target_race_middle_eastern',\n",
       " 'target_race_native_american',\n",
       " 'target_race_pacific_islander',\n",
       " 'target_race_white',\n",
       " 'target_race_other',\n",
       " 'target_race',\n",
       " 'target_religion_atheist',\n",
       " 'target_religion_buddhist',\n",
       " 'target_religion_christian',\n",
       " 'target_religion_hindu',\n",
       " 'target_religion_jewish',\n",
       " 'target_religion_mormon',\n",
       " 'target_religion_muslim',\n",
       " 'target_religion_other',\n",
       " 'target_religion',\n",
       " 'target_origin_immigrant',\n",
       " 'target_origin_migrant_worker',\n",
       " 'target_origin_specific_country',\n",
       " 'target_origin_undocumented',\n",
       " 'target_origin_other',\n",
       " 'target_origin',\n",
       " 'target_gender_men',\n",
       " 'target_gender_non_binary',\n",
       " 'target_gender_transgender_men',\n",
       " 'target_gender_transgender_unspecified',\n",
       " 'target_gender_transgender_women',\n",
       " 'target_gender_women',\n",
       " 'target_gender_other',\n",
       " 'target_gender',\n",
       " 'target_sexuality_bisexual',\n",
       " 'target_sexuality_gay',\n",
       " 'target_sexuality_lesbian',\n",
       " 'target_sexuality_straight',\n",
       " 'target_sexuality_other',\n",
       " 'target_sexuality',\n",
       " 'target_age_children',\n",
       " 'target_age_teenagers',\n",
       " 'target_age_young_adults',\n",
       " 'target_age_middle_aged',\n",
       " 'target_age_seniors',\n",
       " 'target_age_other',\n",
       " 'target_age',\n",
       " 'target_disability_physical',\n",
       " 'target_disability_cognitive',\n",
       " 'target_disability_neurological',\n",
       " 'target_disability_visually_impaired',\n",
       " 'target_disability_hearing_impaired',\n",
       " 'target_disability_unspecific',\n",
       " 'target_disability_other',\n",
       " 'target_disability',\n",
       " 'annotator_gender',\n",
       " 'annotator_trans',\n",
       " 'annotator_educ',\n",
       " 'annotator_income',\n",
       " 'annotator_ideology',\n",
       " 'annotator_gender_men',\n",
       " 'annotator_gender_women',\n",
       " 'annotator_gender_non_binary',\n",
       " 'annotator_gender_prefer_not_to_say',\n",
       " 'annotator_gender_self_describe',\n",
       " 'annotator_transgender',\n",
       " 'annotator_cisgender',\n",
       " 'annotator_transgender_prefer_not_to_say',\n",
       " 'annotator_education_some_high_school',\n",
       " 'annotator_education_high_school_grad',\n",
       " 'annotator_education_some_college',\n",
       " 'annotator_education_college_grad_aa',\n",
       " 'annotator_education_college_grad_ba',\n",
       " 'annotator_education_professional_degree',\n",
       " 'annotator_education_masters',\n",
       " 'annotator_education_phd',\n",
       " 'annotator_income_<10k',\n",
       " 'annotator_income_10k-50k',\n",
       " 'annotator_income_50k-100k',\n",
       " 'annotator_income_100k-200k',\n",
       " 'annotator_income_>200k',\n",
       " 'annotator_ideology_extremeley_conservative',\n",
       " 'annotator_ideology_conservative',\n",
       " 'annotator_ideology_slightly_conservative',\n",
       " 'annotator_ideology_neutral',\n",
       " 'annotator_ideology_slightly_liberal',\n",
       " 'annotator_ideology_liberal',\n",
       " 'annotator_ideology_extremeley_liberal',\n",
       " 'annotator_ideology_no_opinion',\n",
       " 'annotator_race_asian',\n",
       " 'annotator_race_black',\n",
       " 'annotator_race_latinx',\n",
       " 'annotator_race_middle_eastern',\n",
       " 'annotator_race_native_american',\n",
       " 'annotator_race_pacific_islander',\n",
       " 'annotator_race_white',\n",
       " 'annotator_race_other',\n",
       " 'annotator_age',\n",
       " 'annotator_religion_atheist',\n",
       " 'annotator_religion_buddhist',\n",
       " 'annotator_religion_christian',\n",
       " 'annotator_religion_hindu',\n",
       " 'annotator_religion_jewish',\n",
       " 'annotator_religion_mormon',\n",
       " 'annotator_religion_muslim',\n",
       " 'annotator_religion_nothing',\n",
       " 'annotator_religion_other',\n",
       " 'annotator_sexuality_bisexual',\n",
       " 'annotator_sexuality_gay',\n",
       " 'annotator_sexuality_straight',\n",
       " 'annotator_sexuality_other']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = df.columns.values.tolist()\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d82952c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interesting_columns_for_classification(df):\n",
    "    \"\"\"\n",
    "    The function returns  the target columns that fall into one of these categories : \"gender\", \"race\", \"religion\", \"origin\",\"sexuality\",\"age\",\"disability\". Note that in the dataset, there is no profession target.\n",
    "    We will consider that all and only target_race_ falls into race, all and only target_religion_ falls into religion, all and only target_gender falls into gender and same for others.\n",
    "    \"\"\"\n",
    "    all_columns = df.columns.values.tolist()\n",
    "    \n",
    "    interesting_columns_target_race = []\n",
    "    interesting_columns_target_religion = []\n",
    "    interesting_columns_target_gender = []\n",
    "    interesting_columns_target_origin=[]\n",
    "    interesting_columns_target_sexuality=[]\n",
    "    interesting_columns_target_age=[]\n",
    "    interesting_columns_target_disability=[]\n",
    "    \n",
    "    for name_column in all_columns:\n",
    "        if 'target_race' in name_column :\n",
    "            interesting_columns_target_race.append(name_column) \n",
    "        if 'target_religion' in name_column:\n",
    "            interesting_columns_target_religion.append(name_column)\n",
    "        if 'target_gender' in name_column:\n",
    "            interesting_columns_target_gender.append(name_column)\n",
    "        if 'target_origin' in name_column:\n",
    "            interesting_columns_target_origin.append(name_column)\n",
    "        if 'target_sexuality' in name_column:\n",
    "            interesting_columns_target_sexuality.append(name_column)\n",
    "        if 'target_age' in name_column:\n",
    "            interesting_columns_target_age.append(name_column)\n",
    "        if 'target_disability' in name_column:\n",
    "            interesting_columns_target_disability.append(name_column)\n",
    "            \n",
    "    return interesting_columns_target_race,interesting_columns_target_gender,interesting_columns_target_religion,interesting_columns_target_origin,interesting_columns_target_sexuality,interesting_columns_target_age,interesting_columns_target_disability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f8239",
   "metadata": {},
   "source": [
    "#### Create new dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b187cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_bias_for_each_row(df,interesting_columns_target_race,interesting_columns_target_gender,interesting_columns_target_religion,interesting_columns_target_origin,interesting_columns_target_sexuality,interesting_columns_target_age,interesting_columns_target_disability):\n",
    "    \"\"\"\n",
    "    The goal is to classify one comment into one of the following bias : gender, race, profession, religion. \n",
    "    The function takes as parameters a small dataframe where all the id_comments and text are the same, and the columns in the original dataset that we are interested to make this classification. Note that there is no profession target in this dataset. \n",
    "    \n",
    "    We calcule the number of times each target is True for race, gender, religion, origin, age, sexuality or disability.\n",
    "    If we can observe that there is a higner number of times True for origin, age, sexuality or disability than for race, gender, religion, then we skip this comment.\n",
    "    Else,   among race, gender, religion,  we keep only the target that has the highest number and then, the comment will be classified into the bias that corresponds to that target.\n",
    "            In the extreme case, if two targets have the same number or three tragets have the same number, we pick one of the two randomly. \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #This corresponds to how to calculate the number of time each target is True\n",
    "    #there is no interesting columns for profession\n",
    "    df = df.astype(int)\n",
    "    df = df.sum().to_frame().T\n",
    "    target_race = df[interesting_columns_target_race].sum(axis=1)[0] #number of times the target race has been equal to true\n",
    "    target_gender = df[interesting_columns_target_gender].sum(axis=1)[0] #number of times the target gender has been equal to true\n",
    "    target_religion = df[interesting_columns_target_religion].sum(axis=1)[0] #number of times the target religion has been equal to true\n",
    "    target_origin = df[interesting_columns_target_origin].sum(axis=1)[0] #number of times the target origin has been equal to true\n",
    "    target_sexuality = df[interesting_columns_target_sexuality].sum(axis=1)[0] #number of times the target sexuality has been equal to true\n",
    "    target_age = df[interesting_columns_target_age].sum(axis=1)[0] #number of times the target age has been equal to true\n",
    "    target_disability = df[interesting_columns_target_disability].sum(axis=1)[0] #number of times the target disability has been equal to true\n",
    "\n",
    "    if target_race == target_gender == target_religion == 0: #if the three targets have never been equal to true, then we return None and we will skip this comment \n",
    "        return None\n",
    "    \n",
    "    index_target_max = np.argmax([target_race,target_gender,target_religion,target_origin,target_sexuality,target_age,target_disability]) #which target has been equal to true the most frequently\n",
    "    if index_target_max == 0:\n",
    "        target = \"race\"\n",
    "        target_value_max = target_race\n",
    "    elif index_target_max == 1:\n",
    "        target = \"gender\"\n",
    "        target_value_max = target_gender\n",
    "    elif index_target_max == 2:\n",
    "        target = \"religion\"\n",
    "        target_value_max = target_religion\n",
    "    else:\n",
    "        return None #this is the case when one of these : target_origin, target_age, target_sexuality, target_disability has a higher value than one of these : target_gender, target_race, target_religion\n",
    "    \n",
    "    random.seed(42)\n",
    "    if target_race == target_gender == target_religion: #if the three targets have the same number, then we choose one of them randomly\n",
    "        i = random.randrange(3) #so we choose one target randomly\n",
    "        if i==0:\n",
    "            target = \"race\"\n",
    "        elif i==1:\n",
    "            target = \"gender\"\n",
    "        else:\n",
    "            target = \"religion\"\n",
    "            \n",
    "    else:\n",
    "        if target_race == target_gender == target_value_max: #if two targets have the same number, and this number is the highest one, then we choose randomly one of the two.\n",
    "            i = random.randrange(2) #so we choose one of the two targets randomly\n",
    "            if i==0:\n",
    "                target = \"race\"\n",
    "            else:\n",
    "                target = \"gender\"\n",
    "        elif target_race == target_religion == target_value_max: #if two targets have the same number, and this number is the highest one, then we choose randomly one of the two.\n",
    "            i = random.randrange(2) #so we choose one of the two targets randomly\n",
    "            if i==0:\n",
    "                target = \"race\"\n",
    "            else:\n",
    "                target = \"religion\"\n",
    "        elif target_gender == target_religion == target_value_max: #if two targets have the same number, and this number is the highest one, then we choose randomly one of the two.\n",
    "            i = random.randrange(2) #so we choose one of the two targets randomly\n",
    "            if i==0:\n",
    "                target = \"gender\"\n",
    "            else:\n",
    "                target = \"religion\"\n",
    "    \n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f731a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_dataframe(df):\n",
    "    \"\"\"\n",
    "    We create a new dataframe where the columns are id_comment, text, gender, race, religion, profession, hateful, non-hateful, unrelated.\n",
    "    Each id_comment is unique.\n",
    "    \n",
    "    How to choose in which bias the comment falls into ?\n",
    "    \n",
    "    we call the function assign_bias_for_each_row. \n",
    "    \n",
    "    How to choose wheither the comment is hateful ornon-hateful  ?\n",
    "    \n",
    "    We consider that the hate_speech_score is the same for all comments in the original dataset that have the same id_comment.\n",
    "    For one comment, we extract the hate_speech_score. \n",
    "    If this score is strictly higher than -0.8, it is classified as hateful. \n",
    "    If this score is  lower than -0.8, it is classified as non-hateful. \n",
    "       \n",
    "    The function returns the new dataset .\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    interesting_columns_target_race,interesting_columns_target_gender,interesting_columns_target_religion,interesting_columns_target_origin,interesting_columns_target_sexuality,interesting_columns_target_age,interesting_columns_target_disability= interesting_columns_for_classification(df)\n",
    "    interesting_columns = interesting_columns_target_race + interesting_columns_target_religion + interesting_columns_target_gender + interesting_columns_target_origin + interesting_columns_target_sexuality + interesting_columns_target_age + interesting_columns_target_disability\n",
    "    \n",
    "    \n",
    "    all_id_comment = np.unique(df[\"comment_id\"])\n",
    "    \n",
    "    \n",
    "    new_df = pd.DataFrame(columns=[\"comment_id\",\"text\",\"race\",\"gender\",\"profession\",\"religion\",\"hateful\",\"non-hateful\"])\n",
    "    \n",
    "    for id_comment in all_id_comment:\n",
    "        \n",
    "        text = df.loc[df[\"comment_id\"]==id_comment][\"text\"].iloc[0]\n",
    "        hate_speech_score = df.loc[df[\"comment_id\"]==id_comment][\"hate_speech_score\"].iloc[0]\n",
    "        target = assign_bias_for_each_row(df.loc[df[\"comment_id\"]==id_comment][interesting_columns],interesting_columns_target_race,interesting_columns_target_gender,interesting_columns_target_religion,interesting_columns_target_origin,interesting_columns_target_sexuality,interesting_columns_target_age,interesting_columns_target_disability)\n",
    "        \n",
    "        if target != None:\n",
    "            \n",
    "            new_row = pd.DataFrame({'comment_id':[id_comment],'text':[text], 'race':[target=='race'], 'gender':[target=='gender'], 'profession':[target=='profession'], 'religion':[target=='religion'], 'hateful': [hate_speech_score>-0.8], 'non-hateful': [hate_speech_score<=-0.8]})\n",
    "            new_df = pd.concat([new_df, new_row], ignore_index=True)\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a740ffd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>profession</th>\n",
       "      <th>religion</th>\n",
       "      <th>hateful</th>\n",
       "      <th>non-hateful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>White people are trash.</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>I'm not saying you said that, I'm just saying ...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabrice Fabrice is ostensibly black or black/l...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>First off you look cool as fuck! Anyway if we ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Guranteed at least one of these guys has raped...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27839</th>\n",
       "      <td>50062</td>\n",
       "      <td>So smart. They got the most Christian looking ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27840</th>\n",
       "      <td>50065</td>\n",
       "      <td>With all this racism still alive and well in 2...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27841</th>\n",
       "      <td>50068</td>\n",
       "      <td>That Kenyan girl is gorgeous!</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27842</th>\n",
       "      <td>50069</td>\n",
       "      <td>Yikes!!! Bad enough she'shard to look at.every...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27843</th>\n",
       "      <td>50070</td>\n",
       "      <td>Fuck you go die you fucking bitch ugly ass bitch</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27844 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id                                               text   race  \\\n",
       "0              1                            White people are trash.   True   \n",
       "1              3  I'm not saying you said that, I'm just saying ...   True   \n",
       "2              5  Fabrice Fabrice is ostensibly black or black/l...   True   \n",
       "3              6  First off you look cool as fuck! Anyway if we ...  False   \n",
       "4              8  Guranteed at least one of these guys has raped...  False   \n",
       "...          ...                                                ...    ...   \n",
       "27839      50062  So smart. They got the most Christian looking ...  False   \n",
       "27840      50065  With all this racism still alive and well in 2...   True   \n",
       "27841      50068                      That Kenyan girl is gorgeous!  False   \n",
       "27842      50069  Yikes!!! Bad enough she'shard to look at.every...  False   \n",
       "27843      50070   Fuck you go die you fucking bitch ugly ass bitch  False   \n",
       "\n",
       "      gender profession religion hateful non-hateful  \n",
       "0      False      False    False    True       False  \n",
       "1      False      False    False   False        True  \n",
       "2      False      False    False   False        True  \n",
       "3       True      False    False    True       False  \n",
       "4       True      False    False    True       False  \n",
       "...      ...        ...      ...     ...         ...  \n",
       "27839  False      False     True   False        True  \n",
       "27840  False      False    False    True       False  \n",
       "27841   True      False    False   False        True  \n",
       "27842   True      False    False    True       False  \n",
       "27843   True      False    False    True       False  \n",
       "\n",
       "[27844 rows x 8 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = create_new_dataframe(df)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f7a09",
   "metadata": {},
   "source": [
    "#### Add a label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1ec799b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels_column(df):\n",
    "    \"\"\"\n",
    "    The function adds a column column labels to the df.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for idx in df.index: #iterating over the df\n",
    "        \n",
    "        d = df.loc[idx].to_dict() #transform the corresponding row into a dict\n",
    "        bias=\"\"\n",
    "        how_hateful=\"\"\n",
    "        for (key,value) in d.items():\n",
    "            if value == True: \n",
    "                if key == 'hateful' or key =='non-hateful':\n",
    "                    how_hateful = key\n",
    "                else:\n",
    "                    bias = key\n",
    "        labels.append(bias+\"_\"+how_hateful)\n",
    "    \n",
    "    df[\"labels\"] = labels\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b7efd3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = add_labels_column(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa8717",
   "metadata": {},
   "source": [
    "### Cleaning the comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6d36180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rubenhayat/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "af9307f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6304be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text=text.lower() #On met tout en minuscule\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs\n",
    "    #text = html.sub(r'',text) #Removing html tags\n",
    "    \n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p,'') #Removing punctuations\n",
    "        \n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    text = \" \".join(text) #removing stopwords\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = emoji_pattern.sub(r'', text) #Removing emojis\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9978efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['text'] = new_df['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1296b",
   "metadata": {},
   "source": [
    "### Tokenization and Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76f62255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd92eb2808144176b1ee519f81bdc438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c649bc4a269d4f51be86aa99fcad333b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d1f8b6e276497ca8981a958a94c939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a308da00",
   "metadata": {},
   "source": [
    "#### Example for 1st comment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cf7ff2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'white people trash'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "16cbc1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['white', 'people', 'trash']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(new_df['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e2a47f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2317, 2111, 11669]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(new_df['text'].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7767d659",
   "metadata": {},
   "source": [
    "#### defining max_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cc4ee1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_len(df):\n",
    "    max_len = 0\n",
    "    for comment in df['text']:\n",
    "        inputs_id = tokenizer.encode(comment,add_special_tokens=True)\n",
    "        \n",
    "        max_len = max(max_len, len(inputs_id))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e8128d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max_len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0cc28260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([  101,  2317,  2111, 11669,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "inputs_id = []\n",
    "attention_masks = []\n",
    "\n",
    "for comment in new_df['text']:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        comment,\n",
    "                        add_special_tokens = True, #Add CLS and SEP\n",
    "                        max_length = max_len, \n",
    "                        padding = 'max_length',\n",
    "                        return_tensors = 'pt' #return pytorch tensors\n",
    "                    )\n",
    "    inputs_id.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(inputs_id, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d3acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
