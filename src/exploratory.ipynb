{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Mulitclass classification fine tuned bert</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "#Create the BERT model we will use to fine tuned\n",
    "class BERTForFineTuning(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTForFineTuning, self).__init__()\n",
    "        # first layer is the bert\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-cased', output_hidden_states = True)\n",
    "        # apply a dropout\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        # feature bert input is 768 and we want the prediction on the 8 class\n",
    "        self.l3 = torch.nn.Linear(768, 8)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        outputs = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        output_2 = self.l2(outputs.last_hidden_state)\n",
    "        output = self.l3(output_2)\n",
    "        return outputs.hidden_states, output\n",
    "\n",
    "# Dataloader\n",
    "def dataloader(df, val_frac, test_frac, batch_size, max_lenght=None):\n",
    "\n",
    "    if max_lenght is None:\n",
    "        X = list(df['text'])\n",
    "        y = list(df['Class'])\n",
    "    else:\n",
    "        X = list(df['text'][:max_lenght])\n",
    "        y = list(df['Class'][:max_lenght])\n",
    "\n",
    "    # split the data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_frac, stratify=y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=test_frac, stratify=y_train)\n",
    "\n",
    "    # initialize the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize the comment text --> create an object that has free keys : input_ids, attention_mask, \n",
    "    X_train_tokenize = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "    X_val_tokenize = tokenizer(X_val, padding=True, truncation=True, max_length=512)  \n",
    "    X_test_tokenize = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    train_dataset = HateSpeechData(X_train_tokenize, y_train)\n",
    "    val_dataset = HateSpeechData(X_val_tokenize, y_val)\n",
    "    test_dataset = HateSpeechData(X_test_tokenize, y_test)\n",
    "\n",
    "    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle = True, num_workers=0)\n",
    "\n",
    "    return trainloader, validloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "from tokenize_B import tokenize_BERT\n",
    "\n",
    "# import + preprocess the data\n",
    "def preprocessing(df):     \n",
    "    # 0: comment is not hateful\n",
    "    # 1: comment is hateful (target_race)\n",
    "    # 2: comment is hateful (target_religion)\n",
    "    # 3: comment is hateful (target_origin)\n",
    "    # 4: comment is hateful (target_gender)\n",
    "    # 5: comment is hateful (target_sexuality)\n",
    "    # 6: comment is hateful (target_age)\n",
    "    # 7: comment is hateful (target_disability)\n",
    "    l = []\n",
    "    for i in range(len(df)):\n",
    "        list_class = [0] * 8\n",
    "        list_class[int(df['label'].iloc[i])] = 1\n",
    "        l.append(list_class)\n",
    "    \n",
    "    df['Class'] = l\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_class(i):\n",
    "    \"\"\"\n",
    "    Creating a list of lenght 8 with only zeros except from a one at index i\n",
    "    \"\"\"\n",
    "    l = [0] * 8\n",
    "    l[i]=1\n",
    "    return l\n",
    "\n",
    "def get_class(output):\n",
    "    l = []\n",
    "    for pred in output:\n",
    "        class_pred = [0] * 8\n",
    "        idx = np.argmax(pred)\n",
    "        class_pred[idx] = 1.0\n",
    "        l.append(class_pred)\n",
    "    return l\n",
    "\n",
    "# Custome the data for our need\n",
    "class HateSpeechData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        item = {key:torch.tensor(value[index]) for key, value in self.X.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.y[index], dtype=torch.float)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X[\"input_ids\"])\n",
    "    \n",
    "\n",
    "# Dataloader\n",
    "def data_loader(df,batch_size, max_lenght=None):\n",
    "\n",
    "    if max_lenght is None:\n",
    "        X = list(df['text'])\n",
    "        y = list(df['Class'])\n",
    "    else:\n",
    "        X = list(df['text'][:max_lenght])\n",
    "        y = list(df['Class'][:max_lenght])\n",
    "\n",
    "    # initialize the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize the comment text --> create an object that has free keys : input_ids, attention_mask, \n",
    "    X_tokenize = tokenizer(X, padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    dataset = HateSpeechData(X_tokenize, y)\n",
    "\n",
    "    dataloader_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    return dataloader_loader\n",
    "    \n",
    "\n",
    "class BERTForFineTuningtWithPooling(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTForFineTuningtWithPooling, self).__init__()\n",
    "        # first layer is the bert\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "        # apply a dropout\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 8)\n",
    "    \n",
    "    def forward(self, ids, mask):\n",
    "        outputs = self.l1(ids, attention_mask=mask)\n",
    "        pooled_output = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "        output_2 = self.l2(pooled_output)\n",
    "        output = self.l3(output_2)\n",
    "        return outputs.hidden_states, output\n",
    "    \n",
    "# train_data, val_data, test_data = tokenize_BERT()\n",
    "\n",
    "# # import the data\n",
    "df_train = pd.read_csv('../data/train_data.csv')\n",
    "all_labels = df_train['label']\n",
    "WEIGHTS = 1 / (torch.sqrt(torch.unique(torch.tensor(all_labels), return_counts = True)[1])).to('cuda')\n",
    "# df_train = preprocessing(df_train)\n",
    "\n",
    "# df_test = pd.read_csv('../data/test_data.csv')\n",
    "# df_test = preprocessing(df_test)\n",
    "\n",
    "# df_valid = pd.read_csv('../data/val_data.csv')\n",
    "# df_valid = preprocessing(df_valid)\n",
    "\n",
    "# trainloader= data_loader(df_train, batch_size=4)\n",
    "# validloader= data_loader(df_valid, batch_size=4)\n",
    "# testloader= data_loader(df_test, batch_size=4)\n",
    "\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss(pos_weight=WEIGHTS)(outputs, targets)\n",
    "\n",
    "def validation(validation_loader, model_name):\n",
    "    model = BERTForFineTuningtWithPooling()\n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(validation_loader, 0):\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['labels'].to(device, dtype = torch.float)\n",
    "            _,output = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(output).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets\n",
    "\n",
    "def training_model(nb_epochs, train_dataloader, val_dataloader, patience):\n",
    "    \"\"\"\n",
    "    This function trains the model on training data\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = BERTForFineTuningtWithPooling()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            attention_mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            labels = data['labels'].to(device, dtype = torch.float)\n",
    "            \n",
    "             # initialize the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            #forward inputs\n",
    "            _, output = model.forward(ids, attention_mask, token_type_ids)\n",
    "            # define the loss\n",
    "            loss = loss_fn(output, labels)\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "            # print(\"Capturing:\", torch.cuda.is_current_stream_capturing())\n",
    "            optimizer.step()\n",
    "            # add the loss to the running loss\n",
    "            running_loss+=loss.item()\n",
    "            \n",
    "            print('\\rEpoch: {}\\tbatch: {}\\tLoss =  {:.3f}'.format(epoch, i, loss), end=\"\")\n",
    "\n",
    "        running_loss = running_loss / len(train_dataloader)\n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs, targets, val_loss = validation(validation_loader=val_dataloader, model= model)\n",
    "            outputs = get_class(outputs)\n",
    "            outputs = np.array(outputs)\n",
    "            val_accuracy = accuracy_score(targets, outputs)\n",
    "            val_f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "            val_f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "            print(f\"Epoch {epoch+1}: train CE loss = {running_loss}\", \n",
    "                  f\"|| Valid: CE loss = {val_loss}   acc = {val_accuracy}   macro-F1 = {val_f1_score_macro}    micro-F1 = {val_f1_score_micro}\")\n",
    "\n",
    "        # early-stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            dict_model = model.state_dict()\n",
    "            pat = 0\n",
    "        else:\n",
    "            pat += 1\n",
    "            print(\"pat \", pat)\n",
    "            if pat == patience:\n",
    "                print(\"Early Stopping: Validation Loss did not decrease for\", patience, \"epochs.\")\n",
    "                break\n",
    "        \n",
    "        print(\"\\n\")\n",
    "    torch.save(dict_model, 'Fine_Tuned_Bert.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize_B import tokenize_BERT\n",
    "train_data, val_data, test_data = tokenize_BERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = torch.cat((train_data[3], val_data[3], test_data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5918"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_1(tuple):     \n",
    "    # 0: comment is not hateful\n",
    "    # 1: comment is hateful (target_race)\n",
    "    # 2: comment is hateful (target_religion)\n",
    "    # 3: comment is hateful (target_origin)\n",
    "    # 4: comment is hateful (target_gender)\n",
    "    # 5: comment is hateful (target_sexuality)\n",
    "    # 6: comment is hateful (target_age)\n",
    "    # 7: comment is hateful (target_disability)\n",
    "    labels = tuple[3]\n",
    "    l = []\n",
    "    for i in range(len(labels)):\n",
    "        list_class = [0] * 8\n",
    "        list_class[int(labels[i])] = 1\n",
    "        l.append(list_class)\n",
    "    \n",
    "    new_tuple = (tuple[0], tuple[1], tuple[2], torch.tensor(l))\n",
    "    \n",
    "    return new_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = preprocessing_1(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 1, 0,  ..., 0, 0, 0],\n",
       "        [1, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 0, 0],\n",
       "        [0, 1, 0,  ..., 0, 0, 0],\n",
       "        [0, 1, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(validation_loader, model):\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(validation_loader, 0):\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['labels'].to(device, dtype = torch.float)\n",
    "            _,output = model(ids, mask, token_type_ids)\n",
    "            loss = loss_fn(output, targets)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(output).cpu().detach().numpy().tolist())\n",
    "            # add the loss to the running loss\n",
    "            running_loss+=loss.item()\n",
    "\n",
    "    return fin_outputs, fin_targets, running_loss/len(validation_loader)\n",
    "\n",
    "def training_model(nb_epochs, train_dataloader, val_dataloader, patience):\n",
    "    \"\"\"\n",
    "    This function trains the model on training data\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = BERTForFineTuningtWithPooling()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            attention_mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            labels = data['labels'].to(device, dtype = torch.float)\n",
    "            \n",
    "             # initialize the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            #forward inputs\n",
    "            _, output = model.forward(ids, attention_mask, token_type_ids)\n",
    "            # define the loss\n",
    "            loss = loss_fn(output, labels)\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "            # print(\"Capturing:\", torch.cuda.is_current_stream_capturing())\n",
    "            optimizer.step()\n",
    "            # add the loss to the running loss\n",
    "            running_loss+=loss.item()\n",
    "            \n",
    "            print('\\rEpoch: {}\\tbatch: {}\\tLoss =  {:.3f}'.format(epoch, i, loss), end=\"\")\n",
    "\n",
    "        running_loss = running_loss / len(train_dataloader)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs, targets, val_loss = validation(validation_loader=val_dataloader, model= model)\n",
    "            outputs = get_class(outputs)\n",
    "            outputs = np.array(outputs)\n",
    "            val_accuracy = accuracy_score(targets, outputs)\n",
    "            val_f1_score_micro = f1_score(targets, outputs, average='micro')\n",
    "            val_f1_score_macro = f1_score(targets, outputs, average='macro')\n",
    "            print(f\"Epoch {epoch+1}: train CE loss = {running_loss}\", \n",
    "                  f\"|| Valid: CE loss = {val_loss}   acc = {val_accuracy}   macro-F1 = {val_f1_score_macro}    micro-F1 = {val_f1_score_micro}\")\n",
    "\n",
    "        # early-stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            dict_model = model.state_dict()\n",
    "            pat = 0\n",
    "        else:\n",
    "            pat += 1\n",
    "            print(\"pat \", pat)\n",
    "            if pat == patience:\n",
    "                print(\"Early Stopping: Validation Loss did not decrease for\", patience, \"epochs.\")\n",
    "                break\n",
    "        \n",
    "        print(\"\\n\")\n",
    "    torch.save(dict_model, 'Fine_Tuned_Bert.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tbatch: 9549\tLoss =  0.007\n",
      "\n",
      "Epoch 1: train CE loss = 0.013036077102397554 || Valid: CE loss = 0.007797628375654411   acc = 0.6456925675675675   macro-F1 = 0.488740040648217    micro-F1 = 0.6456925675675675\n",
      "\n",
      "\n",
      "Epoch: 1\tbatch: 9549\tLoss =  0.005\n",
      "\n",
      "Epoch 2: train CE loss = 0.007602591407631587 || Valid: CE loss = 0.00873977635637857   acc = 0.7432432432432432   macro-F1 = 0.5283525848603585    micro-F1 = 0.7432432432432431\n",
      "pat  1\n",
      "\n",
      "\n",
      "Epoch: 2\tbatch: 9549\tLoss =  0.003\n",
      "\n",
      "Epoch 3: train CE loss = 0.005819182833571338 || Valid: CE loss = 0.014366051187067028   acc = 0.6980574324324325   macro-F1 = 0.5005929273505167    micro-F1 = 0.6980574324324325\n",
      "pat  2\n",
      "\n",
      "\n",
      "Epoch: 3\tbatch: 9549\tLoss =  0.003\n",
      "\n",
      "Epoch 4: train CE loss = 0.004438413083900731 || Valid: CE loss = 0.021176147788193357   acc = 0.7027027027027027   macro-F1 = 0.4870425113251453    micro-F1 = 0.7027027027027027\n",
      "pat  3\n",
      "Early Stopping: Validation Loss did not decrease for 3 epochs.\n"
     ]
    }
   ],
   "source": [
    "training_model(15, trainloader, validloader, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0197-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
