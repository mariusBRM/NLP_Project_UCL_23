{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "df = pd.read_csv('../data/hate_speech_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [0,1,1,1]\n",
    "l[int(df['label'].iloc[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):     \n",
    "    # 0: comment is not hateful\n",
    "    # 1: comment is hateful (target_race)\n",
    "    # 2: comment is hateful (target_religion)\n",
    "    # 3: comment is hateful (target_origin)\n",
    "    # 4: comment is hateful (target_gender)\n",
    "    # 5: comment is hateful (target_sexuality)\n",
    "    # 6: comment is hateful (target_age)\n",
    "    # 7: comment is hateful (target_disability)\n",
    "    l = []\n",
    "    for i in range(len(df)):\n",
    "        list_class = [0] * 8\n",
    "        list_class[int(df['label'].iloc[i])] = 1\n",
    "        l.append(list_class)\n",
    "    \n",
    "    df['Class'] = l\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_class(i):\n",
    "    \"\"\"\n",
    "    Creating a list of lenght 8 with only zeros except from a one at index i\n",
    "    \"\"\"\n",
    "    l = [0] * 8\n",
    "    l[i]=1\n",
    "    return l\n",
    "    \n",
    "df = preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hate_speech_score</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>White people are trash.</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>You've caught the big gay, it's known to be ai...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>I'm not saying you said that, I'm just saying ...</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Donald Trump. Yeet myself off a building onto ...</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabrice Fabrice is ostensibly black or black/l...</td>\n",
       "      <td>-2.84</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id                                               text  \\\n",
       "0           1                            White people are trash.   \n",
       "1           2  You've caught the big gay, it's known to be ai...   \n",
       "2           3  I'm not saying you said that, I'm just saying ...   \n",
       "3           4  Donald Trump. Yeet myself off a building onto ...   \n",
       "4           5  Fabrice Fabrice is ostensibly black or black/l...   \n",
       "\n",
       "   hate_speech_score  label                     Class  \n",
       "0               0.46      0  [1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1               0.03      0  [1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2              -1.29      0  [1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3              -0.24      0  [1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4              -2.84      0  [1, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              White people are trash.\n",
       "1    You've caught the big gay, it's known to be ai...\n",
       "2    I'm not saying you said that, I'm just saying ...\n",
       "3    Donald Trump. Yeet myself off a building onto ...\n",
       "4    Fabrice Fabrice is ostensibly black or black/l...\n",
       "5    First off you look cool as fuck! Anyway if we ...\n",
       "6    \\*points to posters asking for palestinian rig...\n",
       "7    Guranteed at least one of these guys has raped...\n",
       "8    They'll come back in your plan, also. Plus we ...\n",
       "9                                   eat my fuck, bitch\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXIMUM = 1000\n",
    "\n",
    "X = list(df['text'][:MAXIMUM])\n",
    "y = list(df['label'][:MAXIMUM])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenize = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenize = tokenizer(X_val, padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateSpeechData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        item = {key:torch.tensor(value[index]) for key, value in self.X.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.y[index])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForFineTuning(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTForFineTuning, self).__init__()\n",
    "        # first layer is the bert\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        # apply a dropout\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        # feature bert input is 768 and we want the prediction on the 8 class\n",
    "        self.l3 = torch.nn.Linear(768, 8)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "    \n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "def train(nm_epoch, training_loader):\n",
    "    \n",
    "    model = BERTForFineTuning()\n",
    "    model.to(\"cuda\")\n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=1e-5)\n",
    "\n",
    "    # set the model to training mode\n",
    "    model.train()\n",
    "    for epoch in nm_epoch:\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(training_loader, 0):\n",
    "            ids = data['input_ids']\n",
    "            attention_mask = data['attention_mask']\n",
    "            token_type_ids = data['token_type_ids']\n",
    "            labels = data['labels']\n",
    "\n",
    "            # initialize the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            #forward inputs\n",
    "            outputs = model.forward(ids, attention_mask, token_type_ids)\n",
    "            # define the loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # add the loss to the running loss\n",
    "            running_loss+=loss.item()\n",
    "\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    try:\n",
    "        torch.save(model.state_dict(), 'fine_tuned_bert.pt')\n",
    "        print('Model has been saved !')\n",
    "    except:\n",
    "        print('The model has already been saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HateSpeechData(X_train_tokenize, y_train)\n",
    "val_dataset = HateSpeechData(X_val_tokenize, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    pred, true_y = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=true_y, y_pred=pred)\n",
    "    recall = recall_score(y_true=true_y, y_pred=pred)\n",
    "    f1= f1_score(y_true=true_y, y_pred=pred)\n",
    "    precision= precision_score(y_true=true_y, y_pred=pred)\n",
    "\n",
    "    return {'accuracy' :accuracy, 'recall': recall, 'f1': f1, 'precision': precision }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = TrainingArguments(\n",
    "    output_dir = \"output\", \n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=arguments, \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Mulitclass classification fine tuned bert</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import + preprocess the data\n",
    "def preprocessing(df):     \n",
    "    # 0: comment is not hateful\n",
    "    # 1: comment is hateful (target_race)\n",
    "    # 2: comment is hateful (target_religion)\n",
    "    # 3: comment is hateful (target_origin)\n",
    "    # 4: comment is hateful (target_gender)\n",
    "    # 5: comment is hateful (target_sexuality)\n",
    "    # 6: comment is hateful (target_age)\n",
    "    # 7: comment is hateful (target_disability)\n",
    "    l = []\n",
    "    for i in range(len(df)):\n",
    "        list_class = [0] * 8\n",
    "        list_class[int(df['label'].iloc[i])] = 1\n",
    "        l.append(list_class)\n",
    "    \n",
    "    df['Class'] = l\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_class(i):\n",
    "    \"\"\"\n",
    "    Creating a list of lenght 8 with only zeros except from a one at index i\n",
    "    \"\"\"\n",
    "    l = [0] * 8\n",
    "    l[i]=1\n",
    "    return l\n",
    "\n",
    "\n",
    "\n",
    "# Custome the data for our need\n",
    "class HateSpeechData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        item = {key:torch.tensor(value[index]) for key, value in self.X.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.y[index], dtype=torch.float)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X[\"input_ids\"])\n",
    "    \n",
    "\n",
    "# Dataloader\n",
    "def dataloader(df, val_frac, test_frac, batch_size, max_lenght=None):\n",
    "\n",
    "    if max_lenght is None:\n",
    "        X = list(df['text'])\n",
    "        y = list(df['Class'])\n",
    "    else:\n",
    "        X = list(df['text'][:MAXIMUM])\n",
    "        y = list(df['Class'][:MAXIMUM])\n",
    "\n",
    "    # split the data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_frac, stratify=y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=test_frac, stratify=y_train)\n",
    "\n",
    "    # initialize the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize the comment text --> create an object that has free keys : input_ids, attention_mask, \n",
    "    X_train_tokenize = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "    X_val_tokenize = tokenizer(X_val, padding=True, truncation=True, max_length=512)  \n",
    "    X_test_tokenize = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    train_dataset = HateSpeechData(X_train_tokenize, y_train)\n",
    "    val_dataset = HateSpeechData(X_val_tokenize, y_val)\n",
    "    test_dataset = HateSpeechData(X_test_tokenize, y_test)\n",
    "\n",
    "    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle = True, num_workers=0)\n",
    "\n",
    "    return trainloader, validloader, testloader\n",
    "\n",
    "    \n",
    "#Create the BERT model we will use to fine tuned\n",
    "class BERTForFineTuning(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTForFineTuning, self).__init__()\n",
    "        # first layer is the bert\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        # apply a dropout\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        # feature bert input is 768 and we want the prediction on the 8 class\n",
    "        self.l3 = torch.nn.Linear(768, 8)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "\n",
    "        output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
    "        output_2 = self.l2(output_1[1])\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "    \n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    print(len(targets))\n",
    "    print(len(outputs))\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "def train(nm_epoch, training_loader):\n",
    "    \n",
    "    model = BERTForFineTuning()\n",
    "    # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to('cpu')\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5,capturable=True )\n",
    "\n",
    "    # set the model to training mode\n",
    "    model.train()\n",
    "    for epoch in range(nm_epoch):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(training_loader, 0):\n",
    "            ids = data['input_ids']\n",
    "            attention_mask = data['attention_mask']\n",
    "            token_type_ids = data['token_type_ids']\n",
    "            labels = data['labels']\n",
    "\n",
    "            # initialize the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            #forward inputs\n",
    "            outputs = model.forward(ids, attention_mask, token_type_ids)\n",
    "            # define the loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "            print(\"Capturing:\", torch.cuda.is_current_stream_capturing())\n",
    "            optimizer.step()\n",
    "            # add the loss to the running loss\n",
    "            running_loss+=loss.item()\n",
    "\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    try:\n",
    "        torch.save(model.state_dict(), 'fine_tuned_bert.pt')\n",
    "        print('Model has been saved !')\n",
    "    except:\n",
    "        print('The model has already been saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\mariu/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mariu/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\mariu/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('../data/hate_speech_preprocessed.csv')\n",
    "df = preprocessing(df)\n",
    "trainloader, validloader, testloader = dataloader(df, val_frac=0.2, test_frac=0.3, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\mariu/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\mariu/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mariu\\OneDrive\\Bureau\\UCL\\T2\\NLP\\Project\\NLP_Project_UCL_23\\src\\exploratory.ipynb Cell 21\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mariu/OneDrive/Bureau/UCL/T2/NLP/Project/NLP_Project_UCL_23/src/exploratory.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m m \u001b[39m=\u001b[39m train(\u001b[39m2\u001b[39;49m,trainloader )\n",
      "\u001b[1;32mc:\\Users\\mariu\\OneDrive\\Bureau\\UCL\\T2\\NLP\\Project\\NLP_Project_UCL_23\\src\\exploratory.ipynb Cell 21\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mariu/OneDrive/Bureau/UCL/T2/NLP/Project/NLP_Project_UCL_23/src/exploratory.ipynb#X26sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m \u001b[39m# device = 'cuda' if torch.cuda.is_available() else 'cpu'\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mariu/OneDrive/Bureau/UCL/T2/NLP/Project/NLP_Project_UCL_23/src/exploratory.ipynb#X26sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m model\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/mariu/OneDrive/Bureau/UCL/T2/NLP/Project/NLP_Project_UCL_23/src/exploratory.ipynb#X26sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(params\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39;49mparameters()\u001b[39m.\u001b[39;49mcuda(), lr\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m,capturable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mariu/OneDrive/Bureau/UCL/T2/NLP/Project/NLP_Project_UCL_23/src/exploratory.ipynb#X26sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m \u001b[39m# set the model to training mode\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mariu/OneDrive/Bureau/UCL/T2/NLP/Project/NLP_Project_UCL_23/src/exploratory.ipynb#X26sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'generator' object has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "m = train(2,trainloader )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\mariu/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\mariu/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "Epoch: 0, Loss:  0.7344102263450623\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting CUDA graph capture of step() for an instance of Adam but this instance was constructed with capturable=False.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mariu\\OneDrive\\Bureau\\UCL\\T2\\NLP\\Project\\NLP_Project_UCL_23\\src\\exploratory.ipynb Cell 22\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mariu/OneDrive/Bureau/UCL/T2/NLP/Project/NLP_Project_UCL_23/src/exploratory.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mariu/OneDrive/Bureau/UCL/T2/NLP/Project/NLP_Project_UCL_23/src/exploratory.ipynb#X31sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mariu/OneDrive/Bureau/UCL/T2/NLP/Project/NLP_Project_UCL_23/src/exploratory.ipynb#X31sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     train(epoch)\n",
      "\u001b[1;32mc:\\Users\\mariu\\OneDrive\\Bureau\\UCL\\T2\\NLP\\Project\\NLP_Project_UCL_23\\src\\exploratory.ipynb Cell 22\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mariu/OneDrive/Bureau/UCL/T2/NLP/Project/NLP_Project_UCL_23/src/exploratory.ipynb#X31sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mariu/OneDrive/Bureau/UCL/T2/NLP/Project/NLP_Project_UCL_23/src/exploratory.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mariu/OneDrive/Bureau/UCL/T2/NLP/Project/NLP_Project_UCL_23/src/exploratory.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n",
      "File \u001b[1;32mc:\\Users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages\\torch\\optim\\adam.py:178\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39m@_use_grad_for_differentiable\u001b[39m\n\u001b[0;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, closure\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m, grad_scaler\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    170\u001b[0m     \u001b[39m\"\"\"Performs a single optimization step.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \n\u001b[0;32m    172\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[39m            supplied from ``grad_scaler.step(optimizer)``.\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cuda_graph_capture_health_check()\n\u001b[0;32m    180\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages\\torch\\optim\\optimizer.py:103\u001b[0m, in \u001b[0;36mOptimizer._cuda_graph_capture_health_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m capturing \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_current_stream_capturing()\n\u001b[0;32m    102\u001b[0m \u001b[39mif\u001b[39;00m capturing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAttempting CUDA graph capture of step() for an instance of \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m    104\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m    105\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39m but this instance was constructed with capturable=False.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    108\u001b[0m     (\u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_warned_capturable_if_run_uncaptured\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m))\n\u001b[0;32m    109\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m\"\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    110\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m capturing)\n\u001b[0;32m    111\u001b[0m ):\n\u001b[0;32m    112\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWarning: This instance was constructed with capturable=True, but step() \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m    113\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mis running without CUDA graph capture. If you never intend to graph-capture this \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m    114\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39minstance, capturable=True can impair performance, and you should set capturable=False.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting CUDA graph capture of step() for an instance of Adam but this instance was constructed with capturable=False."
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=1e-5, capturable=False)\n",
    "\n",
    "model_1 = BERTForFineTuning()\n",
    "model_1.to('cpu')\n",
    "\n",
    "def train(epoch):\n",
    "    model_1.train()\n",
    "    for _,data in enumerate(trainloader, 0):\n",
    "        ids = data['input_ids'].to('cpu', dtype = torch.long)\n",
    "        mask = data['attention_mask'].to('cpu', dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to('cpu', dtype = torch.long)\n",
    "        targets = data['labels'].to('cpu', dtype = torch.float)\n",
    "\n",
    "        outputs = model_1(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%5000==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "for epoch in range(2):\n",
    "    train(epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0197-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
